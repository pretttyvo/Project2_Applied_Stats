---
title: "Proj2_EDA_GraceLang"
author: "Grace Lang"
date: "7/11/2020"
output: word_document
---
# Data set
https://archive.ics.uci.edu/ml/datasets/bank%20marketing
# Paper Referenced - that uses the dataset and conclusions
http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf


```{r setup, include=FALSE}
library(naniar)
library(gplots)
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(matlab)
library(MASS)
library(stats)
library(groupdata2)
library(car)

# connection
bank <- read.csv2('../../Data/bank-additional-full.csv')

```

# Initial look of data
```{r}
summary(bank)
head(bank,10)
#pdays
str(bank)
#No variables missing?
gg_miss_var(bank)
```

#####New Variable Generation
```{r, echo=FALSE}
# convert 999 in pdays to 0
bank$pdays_0 <- bank$pdays
bank$pdays_0[bank$pdays_0 == 999] <- 0

#Creating an Age Bucket
bank$agebucket <- with(bank, ifelse(age>79,"80+",ifelse(age>69,"70-79",ifelse(age>59,"60-69"
,ifelse(age>49,"50-59",ifelse(age>39,"40-49", ifelse(age>29,"30-39",ifelse(age>19,"20-29","Less than 20"))))))))
```


### Converting columns to correct types
Convert required data from vector to int
```{r, results="hide", echo=FALSE}
# converting factors to numeric
cols.vector <- c("emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")
length(cols.vector)

# loop through the columns in the list and convert to numeric
for (col.name in cols.vector){
  bank[col.name] <- sapply(bank[col.name], as.numeric )
}

```


```{r pdays}
# does this mean that each individual record is potentially a duplicate contact to the client
# Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.
# Potential for blocking, leveraging pdays --- REPEATED MEASURE
sort(unique(bank$pdays))
x <- table(bank$pdays)
y <- as.data.frame(x)
#all data
plot(y)
#outlier removed
plot(y[c(1:26),])

not_contacted <- y[27,2]/41188
#96% of records have not been contacted before

g<- as.data.frame(table(bank$pdays==999,bank$y))
#Contact Y/N? -- do we want to change to binary ?
#Code 999 as 0, but then interact binary and continuous pdays -- interaction in model = Pdays.cat*Pdays.cont

# ?? WOULD WE EVEN NEED TO SCALE ANY OF THESE CATEGORICAL VARIABLES? Q for Turner

#Reformatting data.frame
h<- unstack(g, Freq~Var2)

row.names(h) <- levels(g$Var1)
h


ggplot(g, aes(x=Var1, y=Freq, fill=Var2)) + geom_bar(stat = "identity") 

```

```{r see individual fields}

sort(unique(bank$previous))
as.data.frame(table(bank$previous))

sort(unique(bank$cons.price.idx))
sort(unique(bank$nr.employed))
sort(unique(bank$cons.conf.idx))

sort(unique(bank$poutcome))
as.data.frame(table(bank$poutcome,bank$y))
```

# Having  issues plotting categorical - not sure what i'm doing wrong here
Need to find a better alternative for these views... what I iterated on wasn't great
```{r}

#Categorical x Categorical
plot(bank$y, bank$job, xlab = "Response", ylab = "job")

#Categorical x Continuous
plot(bank$y, bank$pdays, xlab = "Response", ylab = "pdays")
#plot(bank$y, bank$previous, xlab = "Response", ylab = "previous")
#plot(bank$y, bank$poutcome, xlab = "Response", ylab = "poutcome")
#plot(bank$y, bank$emp.var.rate, xlab = "Response", ylab = "emp.var.rate")
#plot(bank$y, bank$cons.price.idx, xlab = "Response", ylab = "cons.price.idx")
#plot(bank$y, bank$cons.conf.idx, xlab = "Response", ylab = "cons.conf.idx")
#plot(bank$y, bank$euribor3m, xlab = "Response", ylab = "euribor3m")
#plot(bank$y, bank$nr.employed, xlab = "Response", ylab = "nr.employed")


```

```{r}
#Trying to get a pretty visual of some of the datapoints- don't really care for how this looks
ggplot(bank, aes(cons.price.idx,emp.var.rate)) + 
  geom_point() + 
  facet_grid(rows = bank$y)

#Creating a binary field for the Y
bank$binaryY<-factor(ifelse(bank$y=="yes",1,0))

#Looping through all the columns to see distribution
    ## LOOP CURRENTLY NOT WORKING

#loop.vector <- c("age", "duration","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")
#loop.vector <- colnames(bank) ##tried to use them all, throws error
#for (i in loop.vector){
#  out <- t(aggregate(i~bank$binaryY,data=bank,summary))
#  out
#}
```

I don't think these are great visualizations, but leaving them here for reference if we need them later
```{r}
#Categorical Tables
ftable(addmargins(table(bank$job,bank$y)))
#ftable(addmargins(table(bank$marital,bank$y)))
#ftable(addmargins(table(bank$education,bank$y)))
#ftable(addmargins(table(bank$default,bank$y)))
#ftable(addmargins(table(bank$housing,bank$y)))
#ftable(addmargins(table(bank$loan,bank$y)))
#ftable(addmargins(table(bank$contact,bank$y)))
#ftable(addmargins(table(bank$month,bank$y)))
#ftable(addmargins(table(bank$day_of_week,bank$y)))
#ftable(addmargins(table(bank$poutcome,bank$y)))

```

I still need to go through and add the remaining variables
 
# Bank Client Data
```{r EDA by Variable}
attach(bank)
#Proportions of Categorical
    # No: 88.7%
    # Yes: 11.3%

# ^^ those are the averages i used when comparing notes of proportions below



#Age - Continuous
prop.table(table(y,age),2)
plot(y~age,col=c("red","blue"))
#We might benefit from putting these into buckets and then using those buckets to draw conclusions

attach(bank)
prop.table(table(y,agebucket),2)
barplot(prop.table(table(y,agebucket),2),col=c("red","blue"))

#Job - Categorical
#Student & Retired have more Y with 25 & 31% 
prop.table(table(y,job),2)
plot(y~job,col=c("red","blue"))
#table(y,job)

#Marital - Categorical
#Single & Unkown a little higher with 14-15% 
prop.table(table(y,marital),2)
plot(y~marital,col=c("red","blue"))

#Education - Categorical
prop.table(table(y,education),2) #
plot(y~education,col=c("red","blue"))

#Default - (categorical: 'no','yes','unknown')
prop.table(table(y,default),2) #default: has credit in default?
plot(y~default,col=c("red","blue"))

#Housing
prop.table(table(y,housing),2) #no distinguishing, all around 11% 
plot(y~housing,col=c("red","blue"))

#Loan
prop.table(table(y,loan),2) #no distinguishing, all around 11% 
plot(y~loan,col=c("red","blue"))
```

# Related to the last contact of current campaign
```{r}
#Contact Communication Type
prop.table(table(y,contact),2) #15% yes on cellular vs 5% on telephone
plot(y~contact,col=c("red","blue"))

#Month
prop.table(table(y,month),2) #march & december almost have 50% yeses
plot(y~month,col=c("red","blue"))

#Weekday
prop.table(table(y,day_of_week),2) #Tues - Thur have higher yeses - may be interesting to see in line graph
plot(y~day_of_week,col=c("red","blue"))

```

# Other Attributes
```{r}
#campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
plot(campaign~y,col=c("red","blue"))

#pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
plot(pdays~y,col=c("red","blue"))

#previous: number of contacts performed before this campaign and for this client (numeric)
plot(previous~y,col=c("red","blue"))

#poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
prop.table(table(y,poutcome),2)
plot(y~poutcome,col=c("red","blue"))
```

# Social and Economic Context Attributes
I'll come back to this... 
```{r}
#emp.var.rate: employment variation rate - quarterly indicator (numeric)
plot(emp.var.rate~y,col=c("red","blue"))
plot(y~emp.var.rate,col=c("red","blue"))

#cons.price.idx: consumer price index - monthly indicator (numeric)
plot(y~cons.price.idx,col=c("red","blue"))

#cons.conf.idx: consumer confidence index - monthly indicator (numeric)
plot(y~cons.conf.idx,col=c("red","blue"))

#euribor3m: euribor 3 month rate - daily indicator (numeric)
plot(y~euribor3m,col=c("red","blue"))

#nr.employed: number of employees - quarterly indicator (numeric)
plot(y~nr.employed,col=c("red","blue"))

```

# Looking into the test/training split

Down Sampling Pictures: 
https://www.r-bloggers.com/down-sampling-using-random-forests/
```{r jamie}
set.seed(1234)

# set index
bank$ID <- seq.int(nrow(bank))

trainingRowIndex <- sample(1:nrow(bank), 0.5*nrow(bank))  # row indices for training data
trainingData <- bank[trainingRowIndex, ]  # model training data
trainingsData <- downsample(trainingData, cat_col ='y')
summary(trainingsData)
testData  <- bank[-trainingsData$ID, ]   # test data
```


```{r}
set.seed(123)

bank.yes <- subset(bank, y == "yes")
bank.no <- subset(bank, y == "no")

#trying to see if putting 80% of Yeses into training set will help better results for test set 
index.yes<-sample(1:dim(bank.yes)[1],floor(0.80*dim(bank.yes)),replace=F)
train.yes<-bank.yes[index.yes,]
test.yes<-bank.yes[-index.yes,]

#50/50% split of No's
index.no<-sample(1:dim(bank.no)[1],floor(0.5*dim(bank.no)),replace=F)
train.no<-bank.no[index.no,]
test.no<-bank.no[-index.no,]

bank.train <- rbind(train.yes, train.no)
bank.test <- rbind(test.yes, test.no)

#double check to make sure the dimension & % breakouts are correct
table(bank.train$y)
table(bank.test$y)
table(bank$y)

#prop.table(table(y,job),2)

#Downsampling... Is it even needed anymore?
#Train <- downSample(bank,bank$y)


#Index.yes<-which(bank$y="yes")
#train.index<-sample(index.yes,500,replace=F)
#Train<-dat[c(train.index.yes,train.index.no),]

```
# Running initial test in logit model
```{r}
mylogit <- glm(y ~ ., data = bank, family = "binomial") # family indicates that it is a logit 
summary(mylogit)
```
Initial run of logit shows the following variables as significant: 
nr.employed , cons.price.idx , duration , campaign , pdays  , month , contacttelephone , age , job
education , default, poutcome

# Why is VIF squared in logit?
```{r VIF}
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we 
#discussed in class.
mylogit_2 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign + pdays + month + contact + age + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank, family = "binomial")

(vif(mylogit_2)[,3])^2
#removing age & pdays

mylogit_3 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign  + month + contact  + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank, family = "binomial")
(vif(mylogit_3)[,3])^2

```
# Residual Plots
```{r}
#Residual diagnostics can be obtained using
plot(mylogit_2)
plot(mylogit_3)

#The only plot worth examining here is the fourth/final one that allows you to examine levaeage and cooks d.  You read this 
#just like the MLR one.
```

# Looking at accuracy of mylogit_3 
Do I even need to use the step.model if I already used VIF as a CV?
```{r}
#Make predictions
probabilities <- mylogit_3 %>% predict(bank.test, type = "response")
predicted.classes <- ifelse(probabilities < 0.11, "no", "yes")

# Model accuracy
mean(predicted.classes==bank.test$y)
summary(probabilities)

# Create table for conjfusion Matrix
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

#Confusion Matrix
confusionMatrix(predTable$Predicted, reference = predTable$Observed)

#          Reference
#Prediction    no   yes
#       no  15526    90
#       yes  2748   838

#Step Selected Model - run on mylogit, and didnt see the accuracy rate improve
step.model <- mylogit %>% stepAIC(trace=FALSE)
summary(step.model)
probabilities <- step.model %>% predict(bank.test, type = "response")
predicted.classes <- ifelse(probabilities < 0.11, "no", "yes")
mean(predicted.classes==bank.test$y)
summary(probabilities)
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

confusionMatrix(predTable$Predicted, reference = predTable$Observed)
#          Reference
#Prediction    no   yes
#       no  15524    92
#       yes  2750   836
                           
```


# EDA - Do we want to include PCA here?
Probably not from my first pass - can PCAs take categorical vs continuous?

Dummy code categorical into continuous ... then you can use PCA. -- project 2 description
categorical- ordinal can prove to work within the dummy PCA 
```{r PCA}
pc.result<-prcomp(bank[,c(1,11)],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-bank$y

#There is not a clear seperation between these two continuous variables.... the other variables i think are more categorical (cons.price.index, nr.employed, etc...)
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Age & Duration - Bank Data")

ggplot(data = bank, aes(x = age, y = duration)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("Age & Duration - Bank Data")

```



```{r}
### Test feature selection on the training dataset
#-----------------------------------------------------------------------#

# 1. Creating contigency tables for visual analysis
xtabs(~housing + y, data = df)

# 2. Testing feature selection
mylogit <- glm(y ~ ., data = df, family = "binomial") # family indicates that it is a logit 
summary(mylogit)
#confint(mylogit)

library(aod)
# 3. Chi-Squared Test
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 40:42)

# 4. odds ratios only
exp(coef(mylogit))

# 5. odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))

```



