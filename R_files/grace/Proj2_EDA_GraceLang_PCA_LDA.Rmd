---
title: "Proj2_EDA_GraceLang"
author: "Grace Lang"
date: "7/11/2020"
output: word_document
---


```{r setup, include=FALSE}
library(naniar)
library(gplots)
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(matlab)
library(MASS)
library(stats)
library(groupdata2)
library(car)

# connection
bank <- read.csv2('../../Data/bank-additional-full.csv')

```

#####New Variable Generation
```{r, echo=FALSE}
# convert 999 in pdays to 0
bank$pdays_0 <- bank$pdays
bank$pdays_0[bank$pdays_0 == 999] <- 0

#Creating an Age Bucket
bank$agebucket <- with(bank, ifelse(age>79,"80+",ifelse(age>69,"70-79",ifelse(age>59,"60-69"
,ifelse(age>49,"50-59",ifelse(age>39,"40-49", ifelse(age>29,"30-39",ifelse(age>19,"20-29","Less than 20"))))))))
```


### Converting columns to correct types
Convert required data from vector to int
```{r, results="hide", echo=FALSE}
# converting factors to numeric
cols.vector <- c("emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")
length(cols.vector)

# loop through the columns in the list and convert to numeric
for (col.name in cols.vector){
  bank[col.name] <- sapply(bank[col.name], as.numeric )
}

```

# Looking into the test/training split

Down Sampling Pictures: 
https://www.r-bloggers.com/down-sampling-using-random-forests/
```{r jamie}
set.seed(1234)

# set index
bank$ID <- seq.int(nrow(bank))

trainingRowIndex <- sample(1:nrow(bank), 0.5*nrow(bank))  # row indices for training data
trainingData <- bank[trainingRowIndex, ]  # model training data
trainingsData <- downsample(trainingData, cat_col ='y')
#summary(trainingsData)
testData  <- bank[-trainingsData$ID, ]   # test data
```

# Running initial test in logit model
```{r}
mylogit <- glm(y ~ ., data = bank.train, family = "binomial") # family indicates that it is a logit 
#summary(mylogit)
```
Initial run of logit shows the following variables as significant: 
nr.employed , cons.price.idx , duration , campaign , pdays  , month , contacttelephone , age , job
education , default, poutcome

# Why is VIF squared in logit?
```{r VIF}
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we 
#discussed in class.
mylogit_2 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign + pdays + month + contact + age + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")

(vif(mylogit_2)[,3])^2
#removing age & pdays

mylogit_3 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign  + month + contact  + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")
(vif(mylogit_3)[,3])^2

```
# Residual Plots
```{r}
plot(mylogit_2)
plot(mylogit_3)

#The only plot worth examining here is the fourth/final one that allows you to examine levaeage and cooks d.  You read this just like the MLR one.
```
*********************************************************
  ## LOGISTIC REGRESSION, WITH SOME VARIABLES REMOVED
*********************************************************
# Looking at ROC of mylogit_3 
    Do I even need to use the step.model if I already used VIF as a CV?
```{r logistic ROC}
#Make predictions
pred.log <- as.data.frame(predict(mylogit_3, newdata=bank.test))

#capture predictions
pred.log3 <- prediction(pred.log,bank.test$y)

#Plot the ROC curve to determine cutoff
roc.log.perf = performance(pred.log3, measure = "tpr", x.measure = "fpr")
roc.train <- performance(pred.log3, measure = "auc")
roc.train <- roc.train@y.values
plot(roc.log.perf, colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(roc.train[[1]],3), sep = ""))

#determine the cutoff for the confusion matrix
predicted.classes <- ifelse(pred.log < 0.1, "no", "yes") # 0.1= cutoff of confusion matrix

# Model accuracy
mean(predicted.classes==bank.test$y)
summary(pred.log)

# Create table for conjfusion Matrix
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

#Confusion Matrix
confusionMatrix(predTable$Predicted, reference = predTable$Observed)

#          Reference
#Prediction    no   yes
#       no  17592   459
#       yes   682   469
                        
# 94 % Accuracy
            #Sensitivity : 0.9627          
            #Specificity : 0.5054
```

```{r step}
###AIC: 11416
#Call:
#glm(formula = y ~ education + default + contact + month + day_of_week + 
#    duration + campaign + pdays + previous + poutcome + emp.var.rate + 
#    cons.price.idx + cons.conf.idx + euribor3m + nr.employed + 
#    pdays_0 + agebucket + ID, family = "binomial", data = bank.train)

#Step Selected Model - run on mylogit, and didnt see the accuracy rate improve
step.model <- mylogit %>% stepAIC(trace=FALSE)

#Make predictions
pred.step <- as.data.frame(predict(step.model, newdata=bank.test))

#capture predictions
predict.step <- prediction(pred.step,bank.test$y)

#Plot the ROC curve to determine cutoff
roc.step.perf = performance(predict.step, measure = "tpr", x.measure = "fpr")
roc.step.train <- performance(predict.step, measure = "auc")
roc.step.train <- roc.step.train@y.values
plot(roc.step.perf, colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(roc.step.train[[1]],3), sep = ""))

#determine the cutoff for the confusion matrix
predicted.step.classes <- ifelse(pred.log < 0.2, "no", "yes") # 0.1= cutoff of confusion matrix

# Model accuracy
mean(predicted.step.classes==bank.test$y)

# Create table for conjfusion Matrix
predTable.step <- tibble(Predicted = as.factor(predicted.step.classes), Observed = bank.test$y)

#Confusion Matrix
confusionMatrix(predTable.step$Predicted, reference = predTable.step$Observed)

#          Reference
#Prediction    no   yes
#       no  17617   471
#       yes   657   457
                                          
#               Accuracy : 0.9413  
#            Sensitivity : 0.9640          
#            Specificity : 0.4925 
```

# Not sure if i still need this chunk
```{r}
#step(mylogit,
#     direction="forward",
#     test="Chisq",
#     data=bank.test) #why is it not stepping down and removing any predictors?
#hoslem.test(mylogit_3$y, fitted(mylogit_3), g=10) 

#step(mylogit,
#     direction="backward",
#     test="Chisq",
#     data=bank.test) #why is it not stepping down and removing any predictors?

#Step:  AIC=11427
##summary(glm(formula = y ~ age + education + default + contact + month + 
#    day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + 
#    cons.price.idx + nr.employed + pdays_0 + agebucket, family = "binomial", 
#    data = bank.train))

```


```{r test train split}
library(glmnet)
mylogit_train <- glm(y ~ nr.employed + cons.price.idx + duration + campaign  + month + contact  + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")

cvfit <- cv.glmnet(mylogit_train, bank.test, family = "binomial", type.measure = "class", nlambda = 10)
```
****************************************
  ## PCA
****************************************
# Setting up test/training for just the continuous variables to look at PCA
```{r}
test.pca <- testData[,c(1,11:14,16:20)]
train.pca <- trainingsData[,c(1,11:14,16:20)]
```

# EDA - Do we want to include PCA here?
OH? - PCA2 vs PCA4 has a clear seperation, but the ones before don't seem to have such a clear seperation. Is there some interpretation i'm missing here?

```{r PCA}
pc.result<-prcomp(train.pca,scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-trainingsData$y

#Plotting out some PCAs to see if there is any seperation
#There is not a clear seperation between these the PCA continuous variables.
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

#This one shows promise
ggplot(data = pc.scores, aes(x = PC2, y = PC4)) +
  geom_point(aes(col=y), size=0.5)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC4, y = PC5)) +
  geom_point(aes(col=y), size=0.5)+
  ggtitle("PCA of Bank Data")

ggplot(data = bank, aes(x = age, y = duration)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("Age & Duration - Bank Data")

#make points finer
```
# See if PCA prediction is any better within logistic? Do we even do that?
```{r}

```
****************************************
  ## LDA
****************************************
# LDA
Create another competing model using just the continuous predictors and use LDA or QDA
```{r}
#create data for LDA model
test.lda <- testData[,c(1,11:14,16:21)]
train.lda <- trainingsData[,c(1,11:14,16:21)]

# construct the LDA model
mylda <- lda(y~ ., data = train.lda)
pred <- predict(mylda,newdata = test.lda)

#capture predictions
library(ROCR)
pred.lda <- as.data.frame(pred$posterior)
pred.lda <- prediction(pred.lda[,2],test.lda$y)

#Plot the ROC curve
roc.perf = performance(pred.lda, measure = "tpr", x.measure = "fpr")
auc.train <- performance(pred.lda, measure = "auc")
auc.train <- auc.train@y.values
plot(roc.perf, colorize = TRUE)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

#Add in a confusion matrix?
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

#Confusion Matrix
confusionMatrix(predTable$Predicted, reference = predTable$Observed)


#Saving in case i have to use later 
#creating new binary response
#test.lda$response <- if(test.lda$y=="yes"){1}else{0}
#test.lda[test.lda$y == "yes" ,"response"] <- 1 
#test.lda <- subset(test.lda[,c(1:10,12)])
#train.lda$response <- if(train.lda$y=="yes"){1}else{0}
#train.lda[train.lda$y == "yes" ,"response"] <- 1 
#train.lda <- subset(train.lda[,c(1:10,12)])
```


