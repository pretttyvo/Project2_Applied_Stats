---
title: "Proj2_EDA_GraceLang"
author: "Grace Lang"
date: "7/11/2020"
output: word_document
---


```{r setup, include=FALSE}
library(naniar)
library(gplots)
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(matlab)
library(MASS)
library(stats)
library(groupdata2)
library(car)

# connection
bank <- read.csv2('../../Data/bank-additional-full.csv')

```

#####New Variable Generation
```{r, echo=FALSE}
# convert 999 in pdays to 0
bank$pdays_0 <- bank$pdays
bank$pdays_0[bank$pdays_0 == 999] <- 0

#Creating an Age Bucket
bank$agebucket <- with(bank, ifelse(age>79,"80+",ifelse(age>69,"70-79",ifelse(age>59,"60-69"
,ifelse(age>49,"50-59",ifelse(age>39,"40-49", ifelse(age>29,"30-39",ifelse(age>19,"20-29","Less than 20"))))))))
```


### Converting columns to correct types
Convert required data from vector to int
```{r, results="hide", echo=FALSE}
# converting factors to numeric
cols.vector <- c("emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")
length(cols.vector)

# loop through the columns in the list and convert to numeric
for (col.name in cols.vector){
  bank[col.name] <- sapply(bank[col.name], as.numeric )
}

```

# Looking into the test/training split

Down Sampling Pictures: 
https://www.r-bloggers.com/down-sampling-using-random-forests/
```{r jamie}
set.seed(1234)

# set index
bank$ID <- seq.int(nrow(bank))

trainingRowIndex <- sample(1:nrow(bank), 0.5*nrow(bank))  # row indices for training data
trainingData <- bank[trainingRowIndex, ]  # model training data
trainingsData <- downsample(trainingData, cat_col ='y')
summary(trainingsData)
testData  <- bank[-trainingsData$ID, ]   # test data
```


```{r grace test split}
set.seed(123)

bank.yes <- subset(bank, y == "yes")
bank.no <- subset(bank, y == "no")

#trying to see if putting 80% of Yeses into training set will help better results for test set 
index.yes<-sample(1:dim(bank.yes)[1],floor(0.80*dim(bank.yes)),replace=F)
train.yes<-bank.yes[index.yes,]
test.yes<-bank.yes[-index.yes,]

#50/50% split of No's
index.no<-sample(1:dim(bank.no)[1],floor(0.5*dim(bank.no)),replace=F)
train.no<-bank.no[index.no,]
test.no<-bank.no[-index.no,]

bank.train <- rbind(train.yes, train.no)
bank.test <- rbind(test.yes, test.no)

#double check to make sure the dimension & % breakouts are correct
table(bank.train$y)
table(bank.test$y)
table(bank$y)

#prop.table(table(y,job),2)

#Downsampling... Is it even needed anymore?
#Train <- downSample(bank,bank$y)


#Index.yes<-which(bank$y="yes")
#train.index<-sample(index.yes,500,replace=F)
#Train<-dat[c(train.index.yes,train.index.no),]

```
# Running initial test in logit model
```{r}
mylogit <- glm(y ~ ., data = bank.train, family = "binomial") # family indicates that it is a logit 
summary(mylogit)
```
Initial run of logit shows the following variables as significant: 
nr.employed , cons.price.idx , duration , campaign , pdays  , month , contacttelephone , age , job
education , default, poutcome

# Why is VIF squared in logit?
```{r VIF}
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we 
#discussed in class.
mylogit_2 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign + pdays + month + contact + age + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")

(vif(mylogit_2)[,3])^2
#removing age & pdays

mylogit_3 <- glm(y ~ nr.employed + cons.price.idx + duration + campaign  + month + contact  + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")
(vif(mylogit_3)[,3])^2

```
# Residual Plots
```{r}
plot(mylogit_2)
plot(mylogit_3)

#The only plot worth examining here is the fourth/final one that allows you to examine levaeage and cooks d.  You read this just like the MLR one.
```

# Looking at accuracy of mylogit_3 
Do I even need to use the step.model if I already used VIF as a CV?
```{r}
#Make predictions
probabilities <- mylogit_3 %>% predict(bank.test, type = "response")
predicted.classes <- ifelse(probabilities < 0.11, "no", "yes")

# Model accuracy
mean(predicted.classes==bank.test$y)
summary(probabilities)

# Create table for conjfusion Matrix
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

#Confusion Matrix
confusionMatrix(predTable$Predicted, reference = predTable$Observed)

#          Reference
#Prediction    no   yes
#        no  14727    45
#       yes  3547   883
# 81 % Accuracy

#Step Selected Model - run on mylogit, and didnt see the accuracy rate improve
step.model <- mylogit %>% stepAIC(trace=FALSE)
summary(step.model) 

probabilities <- step.model %>% predict(bank.test, type = "response")
predicted.classes <- ifelse(probabilities < 0.11, "no", "yes")
mean(predicted.classes==bank.test$y)
summary(probabilities)
predTable <- tibble(Predicted = as.factor(predicted.classes), Observed = bank.test$y)

confusionMatrix(predTable$Predicted, reference = predTable$Observed)
#          Reference
#Prediction    no   yes
#       no  14721    50
 #      yes  3553   878
         #            .8124  
step(mylogit,
     direction="forward",
     test="Chisq",
     data=bank.test) #why is it not stepping down and removing any predictors?
hoslem.test(mylogit_3$y, fitted(mylogit_3), g=10) 

step(mylogit,
     direction="backward",
     test="Chisq",
     data=bank.test) #why is it not stepping down and removing any predictors?

#Step:  AIC=11427
summary(glm(formula = y ~ age + education + default + contact + month + 
    day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + 
    cons.price.idx + nr.employed + pdays_0 + agebucket, family = "binomial", 
    data = bank.train))
```
```{r test train split}
library(glmnet)
mylogit_train <- glm(y ~ nr.employed + cons.price.idx + duration + campaign  + month + contact  + job +education +default + pdays_0 + agebucket + day_of_week +poutcome, data = bank.train, family = "binomial")

cvfit <- cv.glmnet(mylogit_train, bank.test, family = "binomial", type.measure = "class", nlambda = 10)
```

# Setting up test/training for just the continuous variables to look at PCA
```{r}
test.pca <- testData[,c(1,11:14,16:20)]
train.pca <- trainingsData[,c(1,11:14,16:20)]
```

# EDA - Do we want to include PCA here?
OH? - PCA2 vs PCA4 has a clear seperation, but the ones before don't seem to have such a clear seperation. Is there some interpretation i'm missing here?

```{r PCA}
pc.result<-prcomp(train.pca,scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-trainingsData$y

#Plotting out some PCAs to see if there is any seperation
#There is not a clear seperation between these the PCA continuous variables.
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC1, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC3)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

#This one shows promise
ggplot(data = pc.scores, aes(x = PC2, y = PC4)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = pc.scores, aes(x = PC2, y = PC5)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Bank Data")

ggplot(data = bank, aes(x = age, y = duration)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("Age & Duration - Bank Data")

```
# See if PCA prediction is any better within logistic? Do we even do that?
```{r}

```

# LDA
```{r}
# kept getting errors, need to rewrite

```
```

