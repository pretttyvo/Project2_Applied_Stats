---
title: "Proj2_EDA_GraceLang"
author: "Grace Lang"
date: "7/11/2020"
output: word_document
---
# Data set
https://archive.ics.uci.edu/ml/datasets/bank%20marketing
# Paper Referenced - that uses the dataset and conclusions
http://media.salford-systems.com/video/tutorial/2015/targeted_marketing.pdf

## Response
21 - y - has the client subscribed a term deposit? (binary: 'yes','no')

# Questions about dataset
I'm not sure what the following variables mean in the context of the response:

11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

# other attributes:
12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
14 - previous: number of contacts performed before this campaign and for this client (numeric)
15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')

# social and economic context attributes
16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)
17 - cons.price.idx: consumer price index - monthly indicator (numeric)
18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)
19 - euribor3m: euribor 3 month rate - daily indicator (numeric)
20 - nr.employed: number of employees - quarterly indicator (numeric)



```{r setup, include=FALSE}
library(naniar)
library(gplots)
library(ggplot2)
library(dplyr)
library(caret)
library(reshape2)
library(matlab)
library(MASS)
library(stats)

#Grace's connection
bank <- read.csv2("C:/Users/david/OneDrive/Desktop/AppliedStats/Project2_Applied_Stats/Data/bank-additional-full.csv")

#Julia's connection
#setwd('~/Documents/GitHub/Project2_Applied_Stats/Data/')
#bank <- read.csv2("bank-additional-full.csv",header = TRUE)

#Jamie's connection
#bank <- read.csv2('../../Data/bank-additional-full.csv')

summary(bank)
head(bank,10)
#pdays
str(bank)
#No variables missing?
gg_miss_var(bank)
```

```{r pdays}
# does this mean that each individual record is potentially a duplicate contact to the client
# Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.
# Potential for blocking, leveraging pdays --- REPEATED MEASURE
sort(unique(bank$pdays))
x <- table(bank$pdays)
y <- as.data.frame(x)
#all data
plot(y)
#outlier removed
plot(y[c(1:26),])

not_contacted <- y[27,2]/41188
#96% of records have not been contacted before

g<- as.data.frame(table(bank$pdays==999,bank$y))
#Contact Y/N? -- do we want to change to binary ?
#Code 999 as 0, but then interact binary and continuous pdays -- interaction in model = Pdays.cat*Pdays.cont

# ?? WOULD WE EVEN NEED TO SCALE ANY OF THESE CATEGORICAL VARIABLES? Q for Turner

#Reformatting data.frame
h<- unstack(g, Freq~Var2)

row.names(h) <- levels(g$Var1)
h


ggplot(g, aes(x=Var1, y=Freq, fill=Var2)) + geom_bar(stat = "identity") 

```

```{r previous}

sort(unique(bank$previous))
as.data.frame(table(bank$previous))

sort(unique(bank$cons.price.idx))
sort(unique(bank$nr.employed))
sort(unique(bank$cons.conf.idx))

sort(unique(bank$poutcome))
as.data.frame(table(bank$poutcome,bank$y))
```


# Look into the records where it has unknown - does it have a bunch of unknown in other fields, or is it just in a few (NAs in essence) - Jamie
```{r}

```

# Having  issues plotting categorical - not sure what i'm doing wrong here
Need to find a better alternative for these views... what I iterated on wasn't great
```{r}

#Categorical x Categorical
plot(bank$y, bank$job, xlab = "Response", ylab = "job")

#Categorical x Continuous
plot(bank$y, bank$pdays, xlab = "Response", ylab = "pdays")
#plot(bank$y, bank$previous, xlab = "Response", ylab = "previous")
#plot(bank$y, bank$poutcome, xlab = "Response", ylab = "poutcome")
#plot(bank$y, bank$emp.var.rate, xlab = "Response", ylab = "emp.var.rate")
#plot(bank$y, bank$cons.price.idx, xlab = "Response", ylab = "cons.price.idx")
#plot(bank$y, bank$cons.conf.idx, xlab = "Response", ylab = "cons.conf.idx")
#plot(bank$y, bank$euribor3m, xlab = "Response", ylab = "euribor3m")
#plot(bank$y, bank$nr.employed, xlab = "Response", ylab = "nr.employed")


```

```{r}
#Trying to get a pretty visual of some of the datapoints- don't really care for how this looks
ggplot(bank, aes(cons.price.idx,emp.var.rate)) + 
  geom_point() + 
  facet_grid(rows = bank$y)

#Creating a binary field for the Y
bank$binaryY<-factor(ifelse(bank$y=="yes",1,0))

#Looping through all the columns to see distribution
    ## LOOP CURRENTLY NOT WORKING

#loop.vector <- c("age", "duration","previous","emp.var.rate","cons.price.idx","cons.conf.idx","euribor3m","nr.employed")
#loop.vector <- colnames(bank) ##tried to use them all, throws error
#for (i in loop.vector){
#  out <- t(aggregate(i~bank$binaryY,data=bank,summary))
#  out
#}
```

I don't think these are great visualizations, but leaving them here for reference if we need them later
```{r}
#Categorical Tables
ftable(addmargins(table(bank$job,bank$y)))
#ftable(addmargins(table(bank$marital,bank$y)))
#ftable(addmargins(table(bank$education,bank$y)))
#ftable(addmargins(table(bank$default,bank$y)))
#ftable(addmargins(table(bank$housing,bank$y)))
#ftable(addmargins(table(bank$loan,bank$y)))
#ftable(addmargins(table(bank$contact,bank$y)))
#ftable(addmargins(table(bank$month,bank$y)))
#ftable(addmargins(table(bank$day_of_week,bank$y)))
#ftable(addmargins(table(bank$poutcome,bank$y)))

```

I still need to go through and add the remaining variables
 
# Bank Client Data
```{r EDA by Variable}
attach(bank)
#Proportions of Categorical
    # No: 88.7%
    # Yes: 11.3%

# ^^ those are the averages i used when comparing notes of proportions below



#Age - Continuous
prop.table(table(y,age),2)
plot(y~age,col=c("red","blue"))
#We might benefit from putting these into buckets and then using those buckets to draw conclusions

#Creating an Age Bucket
bank$agebucket <- with(bank, ifelse(age>79,"80+",ifelse(age>69,"70-79",ifelse(age>59,"60-69"
,ifelse(age>49,"50-59",ifelse(age>39,"40-49", ifelse(age>29,"30-39",ifelse(age>19,"20-29","Less than 20"))))))))

attach(bank)
prop.table(table(y,agebucket),2)
barplot(prop.table(table(y,agebucket),2),col=c("red","blue"))

#Job - Categorical
#Student & Retired have more Y with 25 & 31% 
prop.table(table(y,job),2)
plot(y~job,col=c("red","blue"))
#table(y,job)

#Marital - Categorical
#Single & Unkown a little higher with 14-15% 
prop.table(table(y,marital),2)
plot(y~marital,col=c("red","blue"))

#Education - Categorical
prop.table(table(y,education),2) #
plot(y~education,col=c("red","blue"))

#Default - (categorical: 'no','yes','unknown')
prop.table(table(y,default),2) #default: has credit in default?
plot(y~default,col=c("red","blue"))

#Housing
prop.table(table(y,housing),2) #no distinguishing, all around 11% 
plot(y~housing,col=c("red","blue"))

#Loan
prop.table(table(y,loan),2) #no distinguishing, all around 11% 
plot(y~loan,col=c("red","blue"))
```

# Related to the last contact of current campaign
```{r}
#Contact Communication Type
prop.table(table(y,contact),2) #15% yes on cellular vs 5% on telephone
plot(y~contact,col=c("red","blue"))

#Month
prop.table(table(y,month),2) #march & december almost have 50% yeses
plot(y~month,col=c("red","blue"))

#Weekday
prop.table(table(y,day_of_week),2) #Tues - Thur have higher yeses - may be interesting to see in line graph
plot(y~day_of_week,col=c("red","blue"))

```

# Other Attributes
```{r}
#campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)
plot(campaign~y,col=c("red","blue"))

#pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)
plot(pdays~y,col=c("red","blue"))

#previous: number of contacts performed before this campaign and for this client (numeric)
plot(previous~y,col=c("red","blue"))

#poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')
prop.table(table(y,poutcome),2)
plot(y~poutcome,col=c("red","blue"))
```

# Social and Economic Context Attributes
I'll come back to this... 
```{r}
#emp.var.rate: employment variation rate - quarterly indicator (numeric)

#cons.price.idx: consumer price index - monthly indicator (numeric)

#cons.conf.idx: consumer confidence index - monthly indicator (numeric)

#euribor3m: euribor 3 month rate - daily indicator (numeric)

#nr.employed: number of employees - quarterly indicator (numeric)


```

# Looking into the test/training split

Down Sampling Pictures: 
https://www.r-bloggers.com/down-sampling-using-random-forests/

```{r}
set.seed(123)

bank.yes <- subset(bank, y == "yes")
bank.no <- subset(bank, y == "no")

#trying to see if putting 80% of Yeses into training set will help better results for test set 
index.yes<-sample(1:dim(bank.yes)[1],floor(0.80*dim(bank.yes)),replace=F)
train.yes<-bank.yes[index.yes,]
test.yes<-bank.yes[-index.yes,]

#50/50% split of No's
index.no<-sample(1:dim(bank.no)[1],floor(0.5*dim(bank.no)),replace=F)
train.no<-bank.no[index.no,]
test.no<-bank.no[-index.no,]

bank.train <- rbind(train.yes, train.no)
bank.test <- rbind(test.yes, test.no)

#double check to make sure the dimension & % breakouts are correct
table(bank.train$y)
table(bank.test$y)
table(bank$y)

#prop.table(table(y,job),2)

#Downsampling... Is it even needed anymore?
#Train <- downSample(bank,bank$y)


#Index.yes<-which(bank$y="yes")
#train.index<-sample(index.yes,500,replace=F)
#Train<-dat[c(train.index.yes,train.index.no),]

```

# EDA - Do we want to include PCA here?
Probably not from my first pass - can PCAs take categorical vs continuous?

Dummy code categorical into continuous ... then you can use PCA. -- project 2 description
categorical- ordinal can prove to work within the dummy PCA 
```{r PCA}
pc.result<-prcomp(bank[,c(1,11)],scale.=TRUE)
pc.scores<-pc.result$x
pc.scores<-data.frame(pc.scores)
pc.scores$y<-bank$y

#There is not a clear seperation between these two continuous variables.... the other variables i think are more categorical (cons.price.index, nr.employed, etc...)
ggplot(data = pc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("PCA of Age & Duration - Bank Data")

ggplot(data = bank, aes(x = age, y = duration)) +
  geom_point(aes(col=y), size=1)+
  ggtitle("Age & Duration - Bank Data")

```

```{r correlation}
my.cor<-cor(bank[,2:20])

heatmap.2(my.cor,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")
```

# First attempt at fitting the model and seeing the confusion matrix
```{r model step}
#this isn't running because it times out - why? am i including too many variables?
# Full Model
attach(bank.train)
bank.log <- glm(y~. , family = "binomial", data = bank)


 # Step  Model
step.model <- bank.log %>% stepAIC(trace = FALSE)
summary(step.model) 

# Make predictions
predict_1 <- step.model %>% predict(bank.test, type = "response")
predict_2 <- ifelse(predict_1 > 0.14, "yes", "no")

# Model accuracy
mean(predict_2==bank.test$y)
summary(predict_1)

# Confusion Matrix
predTable <- tibble(Predicted = as.factor(predict_2), Observed = bank.test$y)
confusionMatrix(predTable$Predicted, reference = predTable$Observed)
```

# Why is VIF squared in logit?
```{r VIF}
#Using this tool, GVIF is the same as VIF for continuous predictors only
#For categorical predictors, the value GVIG^(1/(2*df)) should be squared and interpreted
#as a usuaul vif type metric.The following code can be used to interpret VIFs like we 
#discussed in class.
(vif(bank.log)[,3])^2

```

